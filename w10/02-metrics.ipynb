{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9DtprsebdLPnJ65xcjvRf",
      "metadata": {},
      "source": [
        "# Evaluating Classifiers\n",
        "This exercise is about evaluation metrics for binary classification. \n",
        "The main focus is to evaluate a classification model, irrespective of which model (linear or non-linear) is being used.\n",
        "<article class=\"message is-info\">\n",
        "  <div class=\"message-header\">Info</div>\n",
        "  <div class=\"message-body\">\n",
        "\n",
        "  Recall that the confusion matrix for binary classification problems has the following structure:\n",
        "\n",
        "$$\n",
        "\\begin{array}{cc|c|c|}\n",
        "  & & \\text{Predicted Negative} & \\text{Predicted Positive} \\\\\n",
        "\\hline\n",
        "\\text{Actual Negative} & & TN & FP \\\\\n",
        "\\hline\n",
        "\\text{Actual Positive} & & FN & TP \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "- **TN** - True Negative\n",
        "- **FN** - False Negative \n",
        "- **FP** - False positive\n",
        "- **TP** - True Positive\n",
        "\n",
        "\n",
        "\n",
        "  </div>\n",
        "</article>\n",
        "\n",
        "<article class=\"message\">\n",
        "    <div class=\"message-body\">\n",
        "        <strong>List of individual tasks</strong>\n",
        "        <ul style=\"list-style: none;\">\n",
        "            <li>\n",
        "            <a href=\"#import\">Task 1: Generating Data</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#First\">Task 2: Training Support Vector Classifier</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#accuracy\">Task 3: Model accuracy</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#classif_report\">Task 4: Performance metrics</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#svc_model\">Task 5: Class Imbalance</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#metrics\">Task 6: ROC and Precision-Recall curves</a>\n",
        "            </li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</article>\n",
        "\n",
        "## Generating Data\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"import\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 1: Generating Data</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "1. Run the cell below to generate a dataset comprising of 500 samples, 20 features, and 2 classes. This dataset is partitioned using an 80-20 train-test split.\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "5pnKNBJiFIYtzXUChmUSS",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_curve, auc, average_precision_score, roc_curve\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "X,Y  = datasets.make_classification(n_samples=500, n_features=20, n_classes=2, random_state=1)\n",
        "print('Dataset Size : ',X.shape,Y.shape)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80, test_size=0.20, stratify=Y, random_state=1)\n",
        "print('Train/Test Size : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gVvvNRVbuzd--mhNfUex2",
      "metadata": {},
      "source": [
        "## Performance Metrics\n",
        "In the following task you will evaluate the model's performance on the test data using different metrics. \n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"First\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 2: Training Support Vector Classifier</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "1. Run the cell below to train a Linear Support Vector Classifier ([`LinearSVC`\n",
        "](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
        ").\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "8UqGGfAd_GfVBTFrmdX_e",
      "metadata": {},
      "source": [
        "classifier1 = LinearSVC(random_state=1, C=0.1)\n",
        "classifier1.fit(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3OWaW_1nKSDZzdD-a9ySr",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"accuracy\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 3: Model accuracy</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "Classification accuracy is given by \n",
        "$$ \\text{accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
        " \n",
        "Run the following cell to make predictions using the `LinearSVC`\n",
        " on the test and training sets. Calculate accuracy by comparing predictions to actual labels: \n",
        "1. Calculate model accuracy on the training set.\n",
        "2. Calculate model accuracy on the test set.\n",
        "3. Run the cell below to construct and plot the confusion matrix of the model predictions on the training set. \n",
        "4. Construct and plot the confusion matrix of the model predictions on the test set. \n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "VGnzdiGZcwQ1vK97RMJZb",
      "metadata": {},
      "source": [
        "Y_preds = classifier1.predict(X_test)\n",
        "Y_preds2 = classifier1.predict(X_train)\n",
        "\n",
        "# Calculate accuracy here ... \n",
        "\n",
        "conf_mat = confusion_matrix(Y_train, Y_preds2)\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2qvN6zEjQUjXuVZfGzlcC",
      "metadata": {},
      "source": [
        "**Performance metrics**\n",
        "<article class=\"message is-info\">\n",
        "  <div class=\"message-header\">Recall from the lecture</div>\n",
        "  <div class=\"message-body\">\n",
        "  \n",
        "  Performance metrics such as **precision**, **recall**, **F$_1$-score**, and **specificity**  provide different views of the performance of the classifier.\n",
        "- **Precision** - or positive predictive value, represents how many predictions of the positive class actually belong to that class. \n",
        "\n",
        "$$\n",
        "\\frac{ùëáùëÉ}{ùëáùëÉ+ùêπùëÉ}\n",
        "$$\n",
        "\n",
        "\n",
        "- **Recall** -  also known as sensitivity, true positive rate, or hit rate assesses whether the classifier correctly identifies positive instances out of the total actual postive instances. \n",
        "\n",
        "$$\n",
        "\\frac{ùëáùëÉ}{ùëáùëÉ+ùêπùëÅ} \n",
        "$$\n",
        "\n",
        "\n",
        "- **F$_1$-score** - harmonic mean of precision & recall. \n",
        "\n",
        "$$\n",
        "2‚àó\\frac{\\text{Precision}‚àó\\text{Recall}}{\\text{Precision}+\\text{Recall}} \n",
        "$$\n",
        "\n",
        "\n",
        "- **Specificity** - also known as the true negative rate, is the percentage of correctly predicted instances of the negative class. \n",
        "\n",
        "$$\n",
        "\\frac{TN}{TN+FP} \n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  </div>\n",
        "</article>\n",
        "\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"classif_report\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 4: Performance metrics</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "1. Use the confusion matrix from [Task 3](#accuracy) to find and store the true positive, false positive, true negative and false negative values. \n",
        "\n",
        "2. In the cell below calculate calculate the following evaluation metrics for the classification model:\n",
        "    - Precision\n",
        "    - Recall\n",
        "    - F1-score\n",
        "    - Specificity\n",
        "\n",
        "\n",
        "3. Inspect the metrics and reflect on how they individually help in understanding and evaluating the performance of a classification model?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "K5m78bduxzp0GorFOpzLl",
      "metadata": {},
      "source": [
        "# write your solution here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "OQd8Ghq7d8t_kY61Ui9iM",
      "metadata": {},
      "source": [
        "## Imbalanced Classes\n",
        "The cell below generates a dataset with 1,000 samples across 10 classes. Then an imbalanced dataset is created by combining 9 classes such that all samples in class 0 are marked as positive, while samples in the remaining classes are marked as negative. This results in a 10% positive and 90% negative distribution.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "OgoOJhFUBLojBCyDvMKZf",
      "metadata": {},
      "source": [
        "X, Y = datasets.make_classification(n_samples=1000, n_classes=10, n_informative=10)\n",
        "\n",
        "# Mark the minority class as True\n",
        "Y = (Y == 0).astype(int)\n",
        "\n",
        "print('Dataset Size:', X.shape, Y.shape)\n",
        "\n",
        "# Check the imbalance ratio\n",
        "imbalance_ratio_actual = np.mean(Y)\n",
        "print(f'Imbalance Ratio (Positive/Minority Class): {imbalance_ratio_actual:.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "vTKAYgTJLL8Mtg8-qBuiD",
      "metadata": {},
      "source": [
        "<article class=\"message task\"><a class=\"anchor\" id=\"svc_model\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 5: Class Imbalance</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights hard\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "The following task will evaluate `classifier2`\n",
        " on the imbalanced dataset.\n",
        "1. The cell below includes the classifier, variables to store performance metrics, and an incomplete for-loop that performs data splitting for 5-fold cross-validation. Complete the loop to:\n",
        "    - Train `classifier2`\n",
        " on the training folds. \n",
        "    - Predict labels on the validation folds.\n",
        "    - Calculate accuracy on the validation folds and store the values.\n",
        "\n",
        "\n",
        "2. Run the cell below to plot the accuracy of the model on each fold. What does the plot tell you about classification performance?\n",
        "\n",
        "3. Extend the loop to calculate the confusion matrix and the performance metrics on the validation sets (precision, recall, F1-Score, specificity). Print the average of each metric (use `np.nanmean()`\n",
        " as some of these metrics might inlcude NaNs). \n",
        "\n",
        "4. Why do some of the metrics return NaNs?\n",
        "\n",
        "5. Is the model able to reliably identify the minority class? What are the implications for the model's performance and its practical utility?\n",
        "\n",
        "6. Run the cell below to plot all the metrics in the same plot. Are certain metrics consistently lower, especially for the minority class? What might this indicate about the model‚Äôs handling of the imbalanced data?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "PjTOOg6NBj4byAyni5f4T",
      "metadata": {},
      "source": [
        "classifier2 = SVC()\n",
        "\n",
        "accuracies= []\n",
        "accuracies, precisions, recalls, f1_scores, specificities = [], [], [], [], []\n",
        "\n",
        "for train_idx_svc, test_idx_svc in KFold(n_splits=5, shuffle=True).split(X):\n",
        "    X_train, X_test = X[train_idx_svc], X[test_idx_svc]\n",
        "    Y_train, Y_test = Y[train_idx_svc], Y[test_idx_svc]\n",
        "\n",
        "    # write your solution here\n",
        "\n",
        "\n",
        "# Step 2\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, 6), accuracies, marker='o')\n",
        "plt.title('Accuracy on Each Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# step 5\n",
        "# Create a list of metric names for plotting\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']\n",
        "\n",
        "# Create a list of the values for each metric\n",
        "metric_values = [accuracies, precisions, recalls, f1_scores, specificities]\n",
        "\n",
        "# Plot the metrics on the same plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "for i, metric_values_list in enumerate(metric_values):\n",
        "    plt.plot(range(1, 6), metric_values_list, marker='o', label=metrics[i])\n",
        "\n",
        "plt.title('Classification Metrics on Each Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "FYYHiO4BRs2H7UN6xBRzl",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "bMd8I4oUR8NiNKLKpo_KD",
      "metadata": {},
      "source": [
        "# write your reflections here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "m8ZCiO1Z9rjnSiSJ0PEhR",
      "metadata": {},
      "source": [
        "## ROC Curves and Precision-Recall Curves\n",
        "This task examines the ROC (Receiver Operating Characteristic) curve and the Precision-Recall curve for the classifier trained on the imbalanced dataset. The ROC curve illustrates the trade-off between true positive rate (sensitivity) and false positive rate, while the Precision-Recall curve shows the balance between precision and recall.\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"metrics\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 6: ROC and Precision-Recall curves</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "The cell below divides the dataset (imbalanced) into a training and a test set. It also calculates both the ROC curve and the Precision-Recall curve and extracts relevant metrics such as: `fpr`\n",
        " (False Positive Rate), `tpr`\n",
        " (True Positive Rate or **recall**), and **precision**.\n",
        "1. Run the cell below to plot the ROC curve and the Precision-Recall curve.\n",
        "2. Use the confusion matrix metrics to argue how the plots provide different insights into the classification model's ability to handle class imbalance? \n",
        "3. Describe when and why you would prioritize one curve over the other when dealing with imbalanced data.\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "FvDGicmhfgZlTa8ovJbKS",
      "metadata": {},
      "source": [
        "X_train, X_test = X[train_idx_svc], X[test_idx_svc]\n",
        "Y_train, Y_test = Y[train_idx_svc], Y[test_idx_svc]\n",
        "\n",
        "# ROC curve\n",
        "decision_function = classifier2.decision_function(X_test)\n",
        "fpr, tpr, _ = roc_curve(Y_test, decision_function)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(Y_test, decision_function)\n",
        "pr_auc = average_precision_score(Y_test, decision_function)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.step(recall, precision, color='b', where='post', lw=2, label='Precision-Recall curve (AUC = %0.2f)' % pr_auc)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "wjrEbb6PrTDceMVlb2zpK",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}