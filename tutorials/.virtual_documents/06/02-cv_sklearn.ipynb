





import scipy.ndimage
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures, Normalizer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate, KFold, RepeatedKFold

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
import numpy as np
import pandas as pd
import seaborn as sns





dataset = fetch_california_housing(as_frame=True)

df = dataset.frame # This is the dataframe (a table)
X = dataset.data # These are the input features (anything but the house price)
y = dataset.target # This contains the output features (just the house price)

print(df.describe()) # This method provides a convenient statistical summary of the columns
print('The dimensions of the dataset are:', df.shape) # Checking the shape of our dataset





cmap = sns.diverging_palette(230, 20, as_cmap=True)
# The Seaborn library makes complex plots like this trivial to implement.
sns.heatmap(dataset.frame.corr(), cmap=cmap)








model = Pipeline([
    ("features", PolynomialFeatures(2)), # Calculates the design matrix for a second order polynomial
    ("normalization", Normalizer()), # Normalizes the features to a (0, 1) range. 
    ("model", LinearRegression()), # The regression model
])





# Fit the model to the dataset
model.fit(dataset.data, dataset.target) 
# Calculate the score on the training data
print('R-squared:',model.score(dataset.data, dataset.target))





X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.3, train_size=0.7)

model.fit(X_train, y_train)
print('R-squared:',model.score(X_test, y_test))





kfold = KFold(100, shuffle=True)

results = cross_validate(
    model,
    dataset.data,
    dataset.target,
    cv=kfold,
    scoring="r2", 
    return_train_score=True,
    return_estimator=True,
)

validation_error = results["test_score"]
print('R-squared:',validation_error.mean())





def k_fold_cv(k):
    kfold = KFold(n_splits=k, shuffle=True)

    return cross_validate(
        model,
        dataset.data,
        dataset.target,
        cv=kfold, # This parameter determines the number of folds
        scoring="r2", 
        return_train_score=True,
        return_estimator=True,
    )

r2_vals = []
r2_std = []
for i in [10, 50, 100, 150, 200]:
    res = k_fold_cv(i)
    r2_vals.append(res["test_score"].mean())
    r2_std.append(res["test_score"].std())


plt.figure(figsize=(10, 6))
plt.errorbar([10, 50,100, 150, 200], r2_vals, yerr=r2_std, fmt='o-', color='r', capsize=5)
plt.title('R2 Score vs. Number of Folds')
plt.xlabel('Number of Folds')
plt.ylabel('R2 Score (Mean)')
plt.xticks([10, 50,100, 150, 200])
plt.grid(True)
plt.show()





# Define the polynomial degrees
degrees = [3, 4, 5]

# Initialize lists to store the model scores for training and testing
train_scores = []
test_scores = []

# Loop through each polynomial degree and fit the model
for degree in degrees:
    model = Pipeline([
        ("features", PolynomialFeatures(degree)),
        ("normalization", Normalizer()),
        ("model", LinearRegression(fit_intercept=True))
    ])
    
    model.fit(X_train, y_train)
    
    # Calculate the R-squared score for training data
    train_score = model.score(X_train, y_train)
    train_scores.append(train_score)
    
    # Calculate the R-squared score for testing data
    test_score = model.score(X_test, y_test)
    test_scores.append(test_score)

    # Calculate standard deviations of training and testing scores
    train_std = np.std(train_scores)
    test_std = np.std(test_scores)

# Create a line plot
plt.figure(figsize=(8, 6))
plt.errorbar(degrees, train_scores, yerr=train_std, label='Training Score', marker='o', linestyle='-', color='b')
plt.title('Model Scores with Standard Deviation')
plt.xlabel('Polynomial Degree')
plt.ylabel('R-squared Score')
plt.legend()
plt.grid(True)
plt.xticks(degrees)
plt.ylim(0.35, 0.9)  # Adjust the y-axis limits if needed
plt.show()





print(test_scores)



