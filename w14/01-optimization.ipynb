{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "css_setup",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atUNHcsRKds0il8GbDmNm",
   "metadata": {},
   "source": [
    "# Non-Linear Optimization: Multivariable Functions\n",
    "In this exercise, the goal is to minimize the function\n",
    "\n",
    "$$\n",
    "f(x, y) = e^{-x^2 - y^2} \\sin(x) \\cos(y),\n",
    "$$\n",
    "which combines exponential decay with sinusoidal variations. This makes it a compelling case for gradient descent optimization.\n",
    "Unlike typical loss functions used in machine learning, where minimization is performed with respect to model parameters and data, here the aim is directly to find the values of $x$ and $y$ that minimize $f(x, y)$. In this context, $f(x, y)$ serves as a measure of error when evaluated with respect to the model parameters. Besides $x$ and $y$ are not treated as model parameters but simply as variables within the mathematical function.\n",
    "## Steps\n",
    "The following steps will be needed\n",
    "- Compute partial derivatives to calculate the gradient in a point.\n",
    "- Calculating forward (prediction) and backward (gradient steps) passes of the function.\n",
    "- Iteratively using the forward and backward passes to optimize the function.\n",
    "\n",
    "\n",
    "<article class=\"message\">\n",
    "    <div class=\"message-body\">\n",
    "        <strong>List of individual tasks</strong>\n",
    "        <ul style=\"list-style: none;\">\n",
    "            <li>\n",
    "            <a href=\"#forward\">Task 1: Forward pass</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#testing\">Task 2: Explain the function</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#partial\">Task 3: Backward pass pen and paper</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#partial1\">Task 4: Backward implementation</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#partial2\">Task 5: Gradient descent</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#plotting\">Task 6: Optimization path</a>\n",
    "            </li>\n",
    "            <li>\n",
    "            <a href=\"#reflection\">Task 7: Reflection</a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</article>\n",
    "\n",
    "## Class Implementation\n",
    "The function $f(x, y)$ and its gradient descent optimization will be implemented within the `ExpTrig`\n",
    " class defined in the cell below\n",
    "The class includes the following methods, which must be implemented in the subsequent tasks.\n",
    "1. `forward`\n",
    "  must return the function value of `f(x,y)`\n",
    " e.g., the predictions\n",
    "2. `df_dx`\n",
    "  must return the partial derivative of the function with respect to `x`\n",
    "\n",
    "3. `df_dy`\n",
    " must return the partial derivative of the function with respect to `y`\n",
    "\n",
    "4. `backward`\n",
    " must return the gradient of `f(x,y)`\n",
    " as a tuple `(df_dx, df_dy)`\n",
    ". The naming 'backward' is used due to its reference to backpropagation.\n",
    "5. `display_function`\n",
    " makes a figure of the function defined in `forward`\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "6GgFwzVWMmZEPRz7ccDkB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "class ExpTrig:\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: x-values (Can be single float or 1D array)\n",
    "        y: y-values (Can be single float or 1D array)\n",
    "\n",
    "        Returns:\n",
    "        The function values of f (size-like x and y)\n",
    "        \"\"\"\n",
    "        # Write the code here\n",
    "\n",
    "    def df_dx(self, x,y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: x-values (Can be single float or 1D numpy array)\n",
    "        y: y-values (Can be single float or 1D numpy array)\n",
    "\n",
    "        Returns:\n",
    "        The partial derivative of f with respect to x (size-like x and y)\n",
    "        \"\"\"\n",
    "        # Write the code here\n",
    "\n",
    "    def df_dy(self, x,y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: x-values (Can be single float or 1D numpy array)\n",
    "        y: y-values (Can be single float or 1D numpy array)\n",
    "\n",
    "        Returns:\n",
    "        The partial derivative of f with respect to y (size-like x and y)\n",
    "        \"\"\"\n",
    "        # Write the code here\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        x: x-values\n",
    "        y: y-values\n",
    "        Returns:\n",
    "        Patial derivatives of the function (i.e. the gradient) as a tuple\n",
    "        \"\"\"\n",
    "        # Write the code here\n",
    "\n",
    "    def display_function(self):\n",
    "\n",
    "        x = np.linspace(-2, 2, 400)\n",
    "        y = np.linspace(-2, 2, 400)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "\n",
    "        z = self.forward(x,y)\n",
    "\n",
    "        # Create a 3D plot\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_surface(x, y, z, cmap='viridis', alpha=0.8)\n",
    "        ax.set_xlabel('X axis')\n",
    "        ax.set_ylabel('Y axis')\n",
    "        ax.set_zlabel('f(x, y)')\n",
    "        ax.set_title('Surface of f(x, y) = e^{-x^2 - y^2} sin(x) cos(y)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P79kNyYqHcwzGZNPj_VfS",
   "metadata": {},
   "source": [
    "<article class=\"message task\"><a class=\"anchor\" id=\"forward\"></a>\n",
    "    <div class=\"message-header\">\n",
    "        <span>Task 1: Forward pass</span>\n",
    "        <span class=\"has-text-right\">\n",
    "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
    "        </span>\n",
    "    </div>\n",
    "<div class=\"message-body\">\n",
    "\n",
    "\n",
    "1. Implement the  `forward`\n",
    " function in the class.\n",
    "2. Run the cell below to visualize the function. \n",
    "\n",
    "\n",
    "\n",
    "</div></article>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "K7ZqwrCWAMA4e6oIgsO3I",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of x, y points\n",
    "f = ExpTrig()\n",
    "\n",
    "f.display_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HD1jMe3bSFZIa5RY4cE7C",
   "metadata": {},
   "source": [
    "<article class=\"message task\"><a class=\"anchor\" id=\"testing\"></a>\n",
    "    <div class=\"message-header\">\n",
    "        <span>Task 2: Explain the function</span>\n",
    "        <span class=\"has-text-right\">\n",
    "          <i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
    "        </span>\n",
    "    </div>\n",
    "<div class=\"message-body\">\n",
    "\n",
    "\n",
    "Use the plot to identify the minima of the function and discuss potential challenges associated with gradient-based optimization.\n",
    "\n",
    "\n",
    "</div></article>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "5XMhVQwfQQ5tOX7hUJVEt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflection here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RzuDor-mIfW6hUle7_2zL",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "**Gradient Descent** is an iterative optimization method that updates input variables step by step by using the gradient of a function with respect to these variables. The updates are made by moving in the negative direction of the gradient, which corresponds to the direction of the steepest decrease in the function's value. The update rule is given by:\n",
    "\n",
    "$$ x_{t+1} = x_t - \\lambda \\nabla_x f(x_t) $$\n",
    "where:\n",
    "- $x_t \\in \\mathbb{R}^n$ represents the input to the function $f(x)$ at iteration $t$,\n",
    "- $x_{t+1}$ is the updated input at iteration $t+1$,\n",
    "- $\\nabla_x f(x_t)$ is the gradient of $f$ with respect to $x$ at $x_t$,\n",
    "- $\\lambda$ is the learning rate, which controls the size of each step, and\n",
    "- $t$ is the current iteration step.\n",
    "\n",
    "<article class=\"message task\"><a class=\"anchor\" id=\"partial\"></a>\n",
    "    <div class=\"message-header\">\n",
    "        <span>Task 3: Backward pass pen and paper</span>\n",
    "        <span class=\"has-text-right\">\n",
    "          <i class=\"bi bi-infinity\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
    "        </span>\n",
    "    </div>\n",
    "<div class=\"message-body\">\n",
    "\n",
    "\n",
    "On a piece of paper, show that the partial derivatives of the function $f(x, y)$ with respect to $x$ and $y$, i.e.,\n",
    "$\\frac{\\partial f}{\\partial x} \\quad \\text{and} \\quad \\frac{\\partial f}{\\partial y}$ are \n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial f}{\\partial x} = -2xe^{-x^2 - y^2} \\sin(x) \\cos(y) + e^{-x^2 - y^2} \\cos(x) \\cos(y) \n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial f}{\\partial y} = -2ye^{-x^2 - y^2} \\sin(x) \\cos(y) - e^{-x^2 - y^2} \\sin(x) \\sin(y) \n",
    "\n",
    "$$\n",
    "<article class=\"message is-warning\">\n",
    "  <div class=\"message-header\">Hint</div>\n",
    "  <div class=\"message-body\">\n",
    "\n",
    "  Recall the _chain rule_ and and _product rule_ for finding the partial derivatives for $x$ and $y$.\n",
    "\n",
    "\n",
    "  </div>\n",
    "</article>\n",
    "\n",
    "\n",
    "</div></article>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "v8WbdleK3dfEKy68tNgp7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HUldgpGrEehYcpqChHLo5",
   "metadata": {},
   "source": [
    "<article class=\"message task\"><a class=\"anchor\" id=\"partial1\"></a>\n",
    "    <div class=\"message-header\">\n",
    "        <span>Task 4: Backward implementation</span>\n",
    "        <span class=\"has-text-right\">\n",
    "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
    "        </span>\n",
    "    </div>\n",
    "<div class=\"message-body\">\n",
    "\n",
    "\n",
    "Implement the following functions in the `ExpTrig`\n",
    " class:\n",
    "1. `df_dx`\n",
    " and `df_dy`\n",
    "  which return the values of the partial derivatives for a given `x`\n",
    " and `y`\n",
    ". See the results in the previous task (in case you didn't complete the previous task the result is given there)\n",
    "\n",
    "\n",
    "2. `backward`\n",
    " method so that it returns the gradient evaluated at a given x, y.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div></article>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "sqPbEXKd7eTP-60-pFQu8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DY_UpvK1PxICibLUjrZfo",
   "metadata": {},
   "source": [
    "## Optimization Method\n",
    "<article class=\"message task\"><a class=\"anchor\" id=\"partial2\"></a>\n",
    "    <div class=\"message-header\">\n",
    "        <span>Task 5: Gradient descent</span>\n",
    "        <span class=\"has-text-right\">\n",
    "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
    "        </span>\n",
    "    </div>\n",
    "<div class=\"message-body\">\n",
    "\n",
    "\n",
    "The following steps outline the implementation of gradient descent for the function $f(x, y)$ , which is defined in the `ExpTrig`\n",
    " class. The implementation of `optimize_function`\n",
    " method involves several tasks that must be completed. Implement each of these steps in the cell below.\n",
    "1. **Initialize Coordinates**: Start by initializing `x`\n",
    " and `y`\n",
    " as `start_x`\n",
    " and `start_y`\n",
    ", respectively. These are your starting guesses for the optimization.\n",
    "\n",
    "2. **Initialize History Lists**: Create two lists, `x_all`\n",
    " and `y_all`\n",
    ", to record the coordinates visited during the optimization.\n",
    "\n",
    "3. **Start Gradient Descent Loop**: Implement a for-loop that iterates a specified number of times. Within each iteration:\n",
    "    - Use the `backward`\n",
    " method of `func`\n",
    " to compute the gradients at the current estimate `(x, y)`\n",
    ".\n",
    "    - Update `x`\n",
    " and `y`\n",
    " using the optimization steps by taking a step in the direction opposite to the gradient, scaled by the `learning_rate`\n",
    ". \n",
    "    - Logging (optional): Add a logging mechanism to display the current state of the optimization. For example, print the iteration number, values `x`\n",
    ", `y`\n",
    ", and the function value, $f(x,y)$ at these coordinates every 25 iterations.\n",
    "    - Record History: append the current values of `x`\n",
    " and `y`\n",
    " to `x_all`\n",
    " and `y_all`\n",
    ", respectively.\n",
    "\n",
    "\n",
    "4. **Return the Result**: the function should return `x_all`\n",
    " and `y_all`\n",
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div></article>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "slxroJU6ol0QBjbdWMBNk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_function(func, start_x, start_y, learning_rate, iterations):\n",
    "    \"\"\"\n",
    "    Optimize a given function using gradient descent.\n",
    "\n",
    "    Args:\n",
    "    - func (Function): A function object that must have 'forward' and 'backward' methods.\n",
    "                       The 'forward' method computes the function value at a given point (x, y),\n",
    "                       and the 'backward' method computes the gradient at that point.\n",
    "    - start_x (float): The starting x-coordinate for the optimization.\n",
    "    - start_y (float): The starting y-coordinate for the optimization.\n",
    "    - learning_rate (float): The step size for each iteration in the gradient descent.\n",
    "    - iterations (int): The total number of iterations for the optimization process.\n",
    "\n",
    "    Returns:\n",
    "    - x_all (list of float): List of all x-coordinates of the points visited during the optimization.\n",
    "    - y_all (list of float): List of all y-coordinates of the points visited during the optimization.\n",
    "\n",
    "    This function performs gradient descent on the provided function object. Starting from\n",
    "    (start_x, start_y), it iteratively moves in the direction opposite to the gradient,\n",
    "    with step sizes determined by the learning rate. The function's value and current\n",
    "    coordinates are printed every 25 iterations. The function returns the history of\n",
    "    coordinates visited during the optimization.\n",
    "    \"\"\"\n",
    "    # return ...\n",
    "    # Write the optimization rutine here\n",
    "\n",
    "# Example optimization\n",
    "x_arr, y_arr = optimize_function(ExpTrig(), start_x=-1.5, start_y=-1., learning_rate=0.2, iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XTurMNvDzV7S6LC548L4z",
   "metadata": {},
   "source": [
    "<article class=\"message task\"><a class=\"anchor\" id=\"plotting\"></a>\n",
    "    <div class=\"message-header\">\n",
    "        <span>Task 6: Optimization path</span>\n",
    "        <span class=\"has-text-right\">\n",
    "          <i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
    "        </span>\n",
    "    </div>\n",
    "<div class=\"message-body\">\n",
    "\n",
    "\n",
    "1. Run the cell below to plot the optimization path of the `optimize_function`\n",
    ".\n",
    "2. Identify areas where the optimization slows down or oscillates and explain why might these occur?\n",
    "3. How might a different learning rate affect the optimization path? Consider the possibilities of paths that overshoot, oscillate, or stagnate.\n",
    "4. How does the shape of the surface (e.g., steep slopes or flat regions) influence the behavior of the optimization path?\n",
    "\n",
    "\n",
    "\n",
    "</div></article>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "x7F935v3sa08j3LTRZo-P",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 400)\n",
    "y = np.linspace(-2, 2, 400)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = ExpTrig().forward(x,y)\n",
    "\n",
    "# Assuming you have lists `x_values` and `y_values` containing the optimization path\n",
    "x_values = np.array(x_arr)\n",
    "y_values = np.array(y_arr)\n",
    "z_values = np.exp(-np.array(x_values)**2 - np.array(y_values)**2) * np.sin(x_values) * np.cos(y_values)\n",
    "\n",
    "# Plot the optimization path on the surface\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap='viridis', alpha=0.6)\n",
    "ax.plot(x_values, y_values, z_values, marker='o', color='r', markersize=5, label='Optimization Path')\n",
    "ax.plot(x_values[-1], y_values[-1], z_values[-1], marker='x', color='b', markersize=10, label='Final Point')\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('f(x, y)')\n",
    "ax.set_title('Optimization Path on f(x, y) Surface')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7CWkHcOumt1xIntfQUcBJ",
   "metadata": {},
   "source": [
    "<article class=\"message task\"><a class=\"anchor\" id=\"reflection\"></a>\n",
    "    <div class=\"message-header\">\n",
    "        <span>Task 7: Reflection</span>\n",
    "        <span class=\"has-text-right\">\n",
    "          <i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
    "        </span>\n",
    "    </div>\n",
    "<div class=\"message-body\">\n",
    "\n",
    "\n",
    "Assess the proficiency of the of the gradient descent algorithm\n",
    "1. Use gradient descent to find the minimum using the following starting points:\n",
    "\n",
    "\n",
    "$$\n",
    "x_{0}=1.5, y_{0}=1.5 \n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{0}=-1.5, y_{0}=-1.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{0}=-1.0, y_{0}=1.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{0}=1.2, y_{0}=-1.5\n",
    "$$\n",
    "2. In the cell below describe the main observations from these optimizations.\n",
    "3. Explain why the optimization process sometimes fails to find the **global** minimum?\n",
    "4. Use different learning rates (try 0.1, 0.5, 1.0) and discuss how they affect the results.\n",
    "5. List two issues commonly encountered in gradient descent optimization and provide a potential solution for each problem:    - Describe each issue and explain why it poses a challenge.\n",
    "    - Propose a potential solution to address each issue effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div></article>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "oGkfygmu5EfKRbRi91c-A",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rX9TnhXTEZ8LP7hOiDPyh",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
