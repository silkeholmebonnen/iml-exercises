{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "UkGeCrfpg6oj5Hq_wIwy-",
      "metadata": {},
      "source": [
        "# Introduction to the exercise\n",
        "In this exercise you will implement and apply PCA to a database consisting of face shapes. The exercise lays the foundations for assignment 2. \n",
        "\n",
        "<article class=\"message\">\n",
        "    <div class=\"message-body\">\n",
        "        <strong>List of individual tasks</strong>\n",
        "        <ul style=\"list-style: none;\">\n",
        "            <li>\n",
        "            <a href=\"#eval\">Task 1: Reflection on reconstruction</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#implementation\">Task 2: Dimensionality reduction and reconstructâ€¦</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#eval2\">Task 3: Evaluating reconstruction error</a>\n",
        "            </li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</article>\n",
        "\n",
        "<article class=\"message is-danger\">\n",
        "  <div class=\"message-header\">Important</div>\n",
        "  <div class=\"message-body\">\n",
        "\n",
        "  This exercise and the in-class exercise are very similar, but use different datasets. In this exercise you should use your implementation from the in-class exercise, but we advise that you actually go through the steps again to understand the algorithm and not only the results.\n",
        "\n",
        "\n",
        "  </div>\n",
        "</article>\n",
        "## Data\n",
        "The dataset used for the assigment consists of 120 landmarks (2D points) of faces (data space). A face consists of 73 (x,y)-coordinate pairs, i.e. 146 featues in total.\n",
        "The following cell imports the necessary libraries, loads the data and uses the function `plot_many_faces`\n",
        " to  visualize 6 faces.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "BuTJjrkpFUBL41mKr2Fq0",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from pca_utils import *\n",
        "\n",
        "path = './db'\n",
        "shapes, images = face_shape_data(path)\n",
        "plot_many_faces(shapes[:6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gUZHiJkkg-5tr4FQK9TtG",
      "metadata": {},
      "source": [
        "## Implementing PCA\n",
        "An application of principal component analysis is about finding a linear projection (transformation)\n",
        "that reduces the number of dimensions used to represent the data while\n",
        "retaining a certain proportion of the total variation. \n",
        "Let $X$ $\\in \\mathbb{R}^{N \\times D}$ be the data matrix, $C$ $\\in \\mathbb{R}^{D \\times D}$ the covariance matrix of $X$, and $V$ $\\in \\mathbb{R}^{D \\times D}$ the matrix of eigenvectors of $C$:\n",
        "\n",
        "$$\n",
        "\n",
        "{V} = \\begin{bmatrix} | & | & & | \\\\ {v}_1 & {v}_2 & \\cdots & {v}_D  \\\\ | & | & & | \\end{bmatrix}.\n",
        "\n",
        "$$\n",
        "Assume, the eigenvectors ${v}_i$ are sorted according to their eigenvalues $\\lambda_i$. The eignevalue of the covariance matrix $\\lambda_i$ gives the variance along the eignevector directions. The sum of all eigenvalues $\\lambda^{(1)}+\\dots+\\lambda^{(D)}$ gives the total variance of the data. \n",
        "(1) **Proportional variance** is the proportion of the total variance explained by a single component. \n",
        "\n",
        "$$\\frac{\\lambda^{(i)}}{\\lambda^{(1)} + \\dots + \\lambda^{(D)}}$$\n",
        "(2) **Cumulative variance** is the cumulative proportion of the total variance explained by the first $k$ components.\n",
        "\n",
        "$$\\frac{\\lambda^{(1)} + \\dots + \\lambda^{(k)}}{\\lambda^{(1)} + \\dots + \\lambda^{(D)}}$$\n",
        "Define $\\Phi_k$, as the $D\\times k$ matrix of the first $k$ eigenvectors of $V$:\n",
        "\n",
        "$$\n",
        "\n",
        "{\\Phi} = \\begin{bmatrix} | & | & & | \\\\ {v}_1 & {v}_2 & \\cdots & {v}_k \\\\ | & | & & | \\end{bmatrix}.\n",
        "\n",
        "$$\n",
        "The column vectors of ${\\Phi_k}$ constitute the orthonormal basis of the latent space. ${\\Phi_k}$ can be used to transform data points between the data space and the latent space. The mapping of a point $x$ from data space to latent space is given by:\n",
        "\n",
        "$$ \n",
        "\\mathbf{a} = \\mathbf{\\Phi_k}^\\top(\\mathbf{x}-\\mathbf{\\mu}),\n",
        "$$\n",
        "and back into data space:\n",
        "\n",
        "$$\n",
        "x  = \\mathbf{\\Phi_k} \\mathbf{a} + \\mathbf{\\mu}\n",
        "$$\n",
        "If $k = D$ (keeping all dimensions) will result in no data loss because it is a change of basis in the same $D$ dimensions. Retaining  $k<D$  principal components and consequently discarding the remaining  $r = D - k$  components, it is effectively assumed that these $r$ components do not contain significant information, e.g. noise. The percentage of the total variance that is removed is given by:\n",
        "\n",
        "$$\\frac{\\lambda^{(D-k)} + \\dots + \\lambda^{(D)}}{\\lambda^{(1)} + \\dots + \\lambda^{(D)}}$$\n",
        "where  $\\lambda^{(i)}$  are the eigenvalues sorted in descending order $( \\lambda^{(1)} \\geq \\lambda^{(2)} \\geq \\dots \\geq \\lambda^{(D)} )$.\n",
        "That is there will be a _reconstruction error_ $\\epsilon = x - \\tilde{x}$,  by first mapping the data $x$ to latent space by $\\mathbf{a} = \\mathbf{\\Phi_k}^\\top(\\mathbf{x}-\\mathbf{\\mu})$ and then  back to data space by $\\tilde{x}  = \\mathbf{\\Phi_k} \\mathbf{a} + \\mathbf{\\mu}$.\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"eval\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 1: Reflection on reconstruction</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-lightbulb-fill\"></i><i class=\"bi bi-stoplights easy\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "1. Why do we get a reconstruction error $\\epsilon$?\n",
        "2. What is the expected error $\\epsilon$ when $k=D$? \n",
        "3. (Optional) Show 2 mathematically. \n",
        "4. How does the expected error change when we decrease $k$?\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "b6ltj5aaSiYWgq8lZ2_OC",
      "metadata": {},
      "source": [
        "# write answers here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cKBoAg8GkKs5UViXYz-ZP",
      "metadata": {},
      "source": [
        "The next task is about implementing PCA and transforming data between data space and latent space.\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"implementation\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 2: Dimensionality reduction and reconstruction</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-code\"></i><i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "1. **Complete PCA:** Following the comments in the function template, complete the `get_principle_components`\n",
        " function below to calculate and return all principle components of the dataset. **Make sure to center the samples (subtract the mean before calculating the covariance matrix)**.\n",
        "\n",
        "2. **Calculate variance:** Complete the `variance_proportion_plot`\n",
        " function according to the comments in the function template to calculate the _proportional_ and _cumulative_ variance. The function already includes code to plot both on a single graph. Once the function is complete, generate the plot to display the _proportional_ and _cumulative_ variances.\n",
        "\n",
        "3. Complete the `transform_to_latent_space`\n",
        " function that transforms the data from the data space to latent space\n",
        "( [Equation 1](#eq:trans) ) \n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        " \\mathbf{a} = \\mathbf{\\Phi}^\\top(\\mathbf{x}-\\mathbf{\\mu})\n",
        "$$\n",
        "4. Complete the `transform_from_latent_space`\n",
        " function that transforms the data from the  latent space to the data space\n",
        "( [Equation 2](#eq:inv) ).\n",
        "\n",
        "\n",
        "$$\n",
        " \\mathbf{x} = \\mathbf{\\Phi} \\mathbf{a} + \\mathbf{\\mu}\n",
        "$$\n",
        "<article class=\"message is-warning\">\n",
        "  <div class=\"message-header\">Tip</div>\n",
        "  <div class=\"message-body\">\n",
        "\n",
        "  Some of the later tasks will be easier if you return all 146 principle components. You can then create another function for extracting $k$ components to generate $\\Phi_k$.\n",
        "\n",
        "\n",
        "  </div>\n",
        "</article>\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "00ck0zE2KTYOaSDTmapFj",
      "metadata": {},
      "source": [
        "## 1\n",
        "\n",
        "def get_principle_components(X):\n",
        "    \"\"\"Calculates principle components for X.\n",
        "\n",
        "    Args:\n",
        "        X: The dataset. An NxD array were N are the number of samples and D are\n",
        "        the number of features.\n",
        "\n",
        "    Returns:\n",
        "        Tuple (components, eigenvalues, mu) where components is a DxD matrix of\n",
        "        principle components, eigenvalues is a D-element vector of\n",
        "        corresponding eigenvalues, and mu is a D-element array containing the mean\n",
        "        vector.\n",
        "    \"\"\"\n",
        "    # Write solutions here\n",
        "    ...\n",
        "\n",
        "    \n",
        "## 2 \n",
        "\n",
        "def variance_proportion_plot(pc_values, max=1):\n",
        "    \"\"\"\n",
        "    Plots the cumulative and individual variance proportions for principal components.\n",
        "\n",
        "    Args:\n",
        "        pc_values: 1D array of eigenvalues representing the variance explained by each principal component.\n",
        "        max: Maximum cumulative variance proportion to display (between 0 and 1).\n",
        "\n",
        "    Returns:\n",
        "        None. Displays a plot of cumulative and individual variance proportions.\n",
        "    \"\"\"\n",
        "    # Calculate total variance, cumulative variance, and individual variance proportions\n",
        "    total = None # write your solution here\n",
        "    cumulative = None # write your solution here\n",
        "    proportion = None # write your solution here\n",
        "    # Individual variance proportion for each component\n",
        "    each = None # write your solution here\n",
        "    \n",
        "\n",
        "    # Find the minimum number of components required to reach the desired cumulative variance\n",
        "    max_idx = np.argwhere(proportion >= max - 1e-7)[0, 0]\n",
        "    \n",
        "    # Set the x-axis values (number of components) up to the determined threshold\n",
        "    x = np.arange(1, max_idx + 1, 1)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure()\n",
        "    plt.xlabel('Number of components')\n",
        "    plt.ylabel('Variance (fraction of total)')\n",
        "    \n",
        "    # Plot cumulative variance proportion and individual variance proportion\n",
        "    plt.plot(x, proportion[:max_idx], 'r-+', label='Cumulative variance')\n",
        "    plt.plot(x, each[:max_idx], 'b-+', label='Variance proportion')\n",
        "\n",
        "    # Set the y-axis limit and add a legend\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend(['Cumulative variance', 'Variance proportion'])\n",
        "    \n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "comp, val, mu = get_principle_components(shapes)\n",
        "variance_proportion_plot(val)\n",
        "\n",
        "\n",
        "## 3\n",
        "\n",
        "def transform_to_latent_space(X, principle_components, mu):\n",
        "    \"\"\"Transforms X to an k-dimensional space where k is the number of\n",
        "    principle_components.\n",
        "\n",
        "    Args:\n",
        "        X: The dataset. An NxD array were N are the number of samples and M are\n",
        "        the number of features.\n",
        "        principle_components: An Dxk matrix containing the principle\n",
        "        components.\n",
        "        mu: A D-element array containing the mean vector.\n",
        "\n",
        "    Returns:\n",
        "        A Nxk array describing the transformed data.\n",
        "    \"\"\"\n",
        "    # Write solutions here\n",
        "    ...\n",
        "\n",
        "\n",
        "## 4\n",
        "\n",
        "def transform_from_latent_space(v, principle_components, mu):\n",
        "    \"\"\"Reverses the dimensionality reduction of v, a Nxk matrix where\n",
        "    k is the number of principle components. The result is a NxD matrix.\n",
        "\n",
        "    Args:\n",
        "        v: The transformed (latent space) dataset with size Nxk.\n",
        "        principle_components: An Dxk matrix containing the principle\n",
        "        components.\n",
        "        mu: A D-element array containing the mean vector.\n",
        "\n",
        "    Returns:\n",
        "        An NxD array reconstruction of the original feature vectors X.\n",
        "    \"\"\"\n",
        "    # Write solutions here\n",
        "    ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "lHVjsDQKuE--8KQXnkB1t",
      "metadata": {},
      "source": [
        "## Reconstruction error\n",
        "This task involves implementing a method to calculate the reconstruction error and using it to examine the impact of varying the number of principal components. The root mean square error (RMSE) is used for calculating the reconstruction error:\n",
        "\n",
        "$$RMSE(x, \\widetilde{x}) = \\sqrt{\\frac{1}{N}\\sum_i (x_i-\\widetilde{x}_i)^2},$$\n",
        "where $x$, $\\widetilde{x}$ are the original and transformed samples\n",
        "respectively and $N$ is the total number of samples $x_i$.\n",
        "<article class=\"message task\"><a class=\"anchor\" id=\"eval2\"></a>\n",
        "    <div class=\"message-header\">\n",
        "        <span>Task 3: Evaluating reconstruction error</span>\n",
        "        <span class=\"has-text-right\">\n",
        "          <i class=\"bi bi-stoplights medium\"></i>\n",
        "        </span>\n",
        "    </div>\n",
        "<div class=\"message-body\">\n",
        "\n",
        "\n",
        "1. **Calculate reconstruction error:** Complete the `reconstruction_error`\n",
        " function according to the comments in the function template to calculate the reconstruction error.\n",
        "\n",
        "2. **Plot reconstruction error:** Use the  `reconstruction_error_plot`\n",
        " function to plot the reconstruction error for $k=1 .... D$. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</div></article>\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "WJfd1nhEhvD5UOc8od8_E",
      "metadata": {},
      "source": [
        "def reconstruction_error(X, principal_components, mu):\n",
        "    \"\"\"\n",
        "    Calculates the reconstruction error after projecting data into a lower-dimensional space\n",
        "    and then reconstructing it back to the original space. \n",
        "\n",
        "    Args:\n",
        "        X: Original data matrix with shape NxD, where N is the number of samples and D is\n",
        "           the number of original features.\n",
        "        principal_components: A Dxk matrix containing the principal components used for \n",
        "           dimensionality reduction, where k is the number of components.\n",
        "        mu: A D-element array representing the mean vector of the original data.\n",
        "\n",
        "    Returns:\n",
        "        The reconstruction error as a single float, representing the root mean squared\n",
        "        error between the original and reconstructed data.\n",
        "    \"\"\"\n",
        "    # Write solutions here\n",
        "    ...\n",
        "\n",
        "def reconstruction_error_plot(X, principal_components, mu):\n",
        "    errs = []\n",
        "    for i in range(len(principal_components)):\n",
        "        errs.append(reconstruction_error(X, principal_components[:, :i], mu))\n",
        "\n",
        "    x = np.arange(1, len(principal_components) + 1, 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Number of components')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.plot(x, errs, 'r-+')\n",
        "    plt.legend(['Reconstruction error (RMSE)'])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "reconstruction_error_plot(shapes, comp, mu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Hzzbm7Y1eI04fnQZ6MZ1C",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}