{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "css_setup",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "HTML(f\"\"\"\n",
    "<style>\n",
    "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MN4TFF85pW5SNiPn_F2D7",
   "metadata": {},
   "source": [
    "# Gaze estimation using Neural Networks\n",
    "## Data preparation\n",
    "This exercise focuses on using a multilayer perceptron (MLP) to estimate gaze using data from week 6 Filtering gaze data\n",
    ".\n",
    "In exercise Filtering gaze data\n",
    " a dictionary was generated dividing the frames into sections, one for each gaze target. \n",
    "\n",
    "---\n",
    "**Task 1 (easy): Load the data 1üë©‚Äçüíª**\n",
    "1. Run the cell below to load a dictionary containing the frame intervals for each target.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yPfMfpo43EoMM6fKVVw1S",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '95.43401336669922'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mnn_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/frames.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/cs/1sem/iml/iml-exercises/w13/nn_util.py:30\u001b[0m, in \u001b[0;36mload_frames\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     29\u001b[0m         key \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m         start, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, row[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     31\u001b[0m         loaded_dict[key] \u001b[38;5;241m=\u001b[39m (start, end)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDictionary loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '95.43401336669922'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nn_util\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "frames = nn_util.load_frames(\"data/frames.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JSFdlOQsxzBvDO_UKN7nQ",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 2 (easy): Load the data 2üë©‚Äçüíª**\n",
    "1. Run the cell below to load the cleaned pupil coordinates in the file `cleaned_pupil_coordinates.csv`\n",
    " and screen coordinates in the file `screen_coordinates.csv`\n",
    " for the `grid`\n",
    " pattern. You may have to change the filepath. The function `map_coordinates_to_targets`\n",
    " returns two $N \\times 2$ arrays containing inputs and labels. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "_NlLrq9k5cKT4v2g85QmL",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_pupil = '../W06/data/output/test_subject_3/grid/cleaned_pupil_coordinates.csv'\n",
    "file_name_screen = '../W06/data/output/test_subject_3/grid/screen_coordinates.csv'\n",
    "pupil_coor = np.asarray(nn_util.load_coordinates(file_name_pupil))\n",
    "screen_coor = np.asarray(nn_util.load_coordinates(file_name_screen))\n",
    "input, labels = nn_util.map_coordinates_to_targets(pupil_coor, frames, screen_coor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-yhX805S7WWkKq3RU-k4F",
   "metadata": {},
   "source": [
    "The data set is divided into training and test data using train_test_split\n",
    " function from scikit-learn.\n",
    "\n",
    "---\n",
    "**Task 3 (easy): Prepare dataüë©‚Äçüíª**\n",
    "In the cell below:\n",
    "1. Use the function `train_test_split`\n",
    " to split the input and target data into a $80\\%/20\\%$ train/test sets.\n",
    "2. Use the function `train_test_split`\n",
    " to split the training into a $75\\%/25\\%$ train/validation sets.\n",
    "3. Visualize the data using the function `plot_data_splits`\n",
    " from the `nn_util.py`\n",
    " file.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "nfNjwavj-vCZ_0CNeYPG2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_util.plot_data_splits(X_train, X_val, X_test) # uncomment once the splits are made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iaT3FF8V6v7JyhjWTzykq",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 4 (easy): Reflection on data splitüí°**\n",
    "1. Reflect on the benefits of making these splits and identify potential pitfals?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "-JZlsit_EQNzz_BG1wcY4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your reflection here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ivteS6554nof-lmTpxj",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "The following tasks introduces an affine neural network but uses non-linear optimization to find the model parameters.\n",
    "\n",
    "---\n",
    "**Task 5 (easy): Linear Least Squareüë©‚Äçüíª**\n",
    "In Assignment 1 Gaze Estimation\n",
    " you used the Linear Least Square for finding the model parameters.\n",
    "1. Run the cell below to learn the model parameters using Linear Least Squares on the entire gaze data training set and visualize the result.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "7NU6mqWTuguMP9fj7CVWZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_util.plot_least_square_results(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dUyI3sngh_uhcoc5EbV7_",
   "metadata": {},
   "source": [
    "The cell below contains the definition of an affine model in Pytorch.\n",
    "\n",
    "---\n",
    "**Task 6 (easy): Train a linear model (gaze data)üë©‚Äçüíª**\n",
    "1. On a piece of paper draw the architecture of the network given the class definition `LinearModel`\n",
    ".\n",
    "**Note:** The class `MSELoss`\n",
    " explicitly defines the _Mean Squared Error_ loss function, for pedagogical reasons. Note, the Pytorch library has its own mse loss\n",
    ".\n",
    "\n",
    "\n",
    "2. Run the cell below to train the network.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "iphFkr-7nyeJrX_p4fotg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        output_dim (int): Number of output features.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Passes the input through the linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, output_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        \"\"\"Args:\n",
    "        x (Tensor): Input tensor.\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the linear transformation.\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "    \n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MSELoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        squared_diff = (input - target) ** 2\n",
    "        if self.reduction == 'mean':\n",
    "            return squared_diff.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return squared_diff.sum()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reduction type. Use 'mean' or 'sum'.\")\n",
    "\n",
    "    \n",
    "def train_model(model, criterion, optimizer, X_train, Y_train, X_val=None, Y_val=None, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        criterion (nn.Module): The loss function to minimize.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        X_train (Nx2 Tensor): Training input data.\n",
    "        Y_train (Nx2 Tensor): Training target data.\n",
    "        X_val (Nx2 Tensor, optional): Validation input data. Defaults to None.\n",
    "        Y_val (Nx2 Tensor, optional): Validation target data. Defaults to None.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        list: Loss values for each epoch (training).\n",
    "        list: Loss values for each epoch (validation).\n",
    "        float: Training time.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    model_params = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, Y_train)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Validation phase (if validation data is provided)\n",
    "        if X_val is not None and Y_val is not None:\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                model_params.append(model.parameters())\n",
    "                val_loss = criterion(val_outputs, Y_val)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return train_losses, val_losses, training_time\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a trained model on test data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained neural network model.\n",
    "        X_test (Tensor): Test input data.\n",
    "        Y_test (Tensor): Test target data.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean squared error (MSE) over the test set.\n",
    "        np.ndarray: Predicted values as a numpy array.\n",
    "        np.ndarray: True values as a numpy array.\n",
    "        np.ndarray: Absolute errors for x and y coordinates.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test)\n",
    "        mse = mean_squared_error(Y_test.cpu().numpy(), test_output.cpu().numpy())\n",
    "        predictions = test_output.cpu().numpy()\n",
    "        true_values = Y_test.cpu().numpy()\n",
    "        errors = np.abs(true_values - predictions)\n",
    "\n",
    "    return mse, predictions, true_values, errors\n",
    "\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "learning_rate = 0.1\n",
    "epoch = 20000\n",
    "\n",
    "# Generate training data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)  \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)  \n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)   \n",
    "\n",
    "model = LinearModel(input_dim, output_dim)\n",
    "criterion = MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "losses, val_losses, training_time = train_model(model, criterion, optimizer, X_train_tensor, Y_train_tensor, X_val_tensor, Y_val_tensor, num_epochs=epoch)\n",
    "\n",
    "# Test the model\n",
    "mse, Y_pred, true_values, errors_nn = test_model(model, X_test_tensor, Y_test_tensor)\n",
    "print(f'Average MSE: {mse}')\n",
    "\n",
    "# Visualize results\n",
    "nn_util.plot_results(\n",
    "        X_train_tensor,\n",
    "        Y_train_tensor,\n",
    "        X_test_tensor,\n",
    "        Y_test_tensor,\n",
    "        Y_pred,\n",
    "        errors_nn,\n",
    "        losses,\n",
    "        val_losses,\n",
    "        model_name='NN',\n",
    "        training_time=training_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tz_go1vFLX1gro6Zn-Hem",
   "metadata": {},
   "source": [
    "You will notice, that the neural network has a difficulty in predicting gaze compared to the linear least square optimization. \n",
    "\n",
    "---\n",
    "**Task 7 (easy): Analyse resultsüí°**\n",
    "1. Provide at least 3 reasons to why the neural network performs worse compared to the linear least squares. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "F9Ewi8kFTxumP4kMK5bw4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qDRsxG6Fn_e5hlKbzLDnU",
   "metadata": {},
   "source": [
    "## Improving performance\n",
    "The following steps will investigate reasons for the poorer performance and include:\n",
    "- Outliers\n",
    "- Preprocessing of the data\n",
    "- The learning rate\n",
    "- The number of iterations\n",
    "\n",
    "### Outliers\n",
    "The following tasks investigate the impact of outliers by analyzing a synthetic dataset with a bit of noise.\n",
    "The function `generate_data_grid`\n",
    " returns a synthetic noisy dataset without outliers.\n",
    "\n",
    "---\n",
    "**Task 8 (easy): Train a linear model (synthetic gaze data)üë©‚Äçüíª**\n",
    "1. Use the function `train_test_split`\n",
    " to split the synthetic data into $80\\%/20\\%$ train/test datasets.\n",
    "2. Split the synthetic training data into a $75\\%/25\\%$ train/validation sets.\n",
    "3. Train an affine model using the synthetic data. \n",
    "4. Calculate the MSE to evaluate model performance. \n",
    "5. Visualize the result using the function `plot_results`\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "FO-77t-GTNmsbvrJmFPql",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_syn, target_syn, A, b = nn_util.generate_data_grid(noise_std=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dkxmRLHIIfaHvEZRCTQ",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 9 (easy): Analyse resultsüí°**\n",
    "1. Consider the reasons why the neural network continues to perform poorly, even when working with synthetic (ideal) data.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "piovAJGDAsMXEo1MoR-k9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LZmKLAYb2L8J1rcjQnAyD",
   "metadata": {},
   "source": [
    "### Data wrangling\n",
    "The following step investigate the impact of preprocessing of the data by normalizing the input and label data. It also investigates the impact of the learning rate and the number of iterations.\n",
    "\n",
    "**Data structure for plotting**\n",
    "To investigate the performance of the models the function `plot_results_collected`\n",
    " from the file `nn_util.py`\n",
    " is used. This function takes six dictionaries as input:\n",
    "- object of model instance:\n",
    "- list of training losses\n",
    "- list of validation losses\n",
    "- Training time: float\n",
    "- $N \\times 2$ array of predictions on test data\n",
    "- list of prediction errors\n",
    "\n",
    "The data needed to populate these data structures were provided gradually througout the exercise. It is important to maintain the key names for the specific models. Define key names such as: `'Synthetic lr: 0.01, epoch: 500'`\n",
    " to indicate architecture and training parameters.\n",
    "\n",
    "\n",
    "---\n",
    "**Task 10 (easy): Train a linear model (normalized gaze data and synthetid gaze data)üë©‚Äçüíª**\n",
    "1. Complete the `DataScaler`\n",
    " class by implementing the `normalize`\n",
    " function and the `denormalize`\n",
    " function. \n",
    "\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{\\text{denormalized}} = x_{\\text{normalized}} \\cdot (x_{\\max} - x_{\\min}) + x_{\\min}\n",
    "$$\n",
    "2. Use `DataScaler`\n",
    "to normalize the data in the `cleaned_pupil.csv`\n",
    " and `screen_coordinates.csv`\n",
    " files.\n",
    "3. Use `DataScaler`\n",
    "to normalize the synthetic data. \n",
    "4. In the nested for-loops:    - Train two models, one for each dataset using the `train_model`\n",
    " function.\n",
    "    - Test the models using the `test_model`\n",
    " function.\n",
    "    - For each model store results in the designated dictionaries:        - Model instance (`LinearModel`\n",
    ")\n",
    "        - Loss (training)\n",
    "        - Loss (validation)\n",
    "        - Training time\n",
    "        - Predictions for test data \n",
    "        - Prediction errors \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5. Use the function `plot_results_collected`\n",
    " from the `nn_util.py`\n",
    " file, to visualize the result.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "bh-maUitEXdjEn35IsrwR",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScaler:\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def normalize(self, data):\n",
    "        return normalized_data\n",
    "\n",
    "    def denormalize(self, normalized_data):\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "learning_rate = [0.0001, 0.1, 1.5]\n",
    "epoch = [500, 2000, 10000]\n",
    "criterion = MSELoss()\n",
    "\n",
    "# Containers gaze data\n",
    "models_dict = {}\n",
    "losses_dict = {}\n",
    "losses_val_dict = {}\n",
    "training_time_dict = {}\n",
    "pred_norm_dict = {}\n",
    "errors_norm_dict = {}\n",
    "mse_norm_dict= {}\n",
    "\n",
    "# Containers synthetic gaze data\n",
    "models_dict_syn = {}\n",
    "losses_dict_syn = {}\n",
    "losses_val_dict_syn = {}\n",
    "training_time_dict_syn = {}\n",
    "pred_norm_dict_syn = {}\n",
    "errors_norm_dict_syn = {}\n",
    "mse_norm_syn_dict= {}\n",
    "\n",
    "\n",
    "#Train the models\n",
    "for i in learning_rate:\n",
    "    for j in epoch:\n",
    "\n",
    "\n",
    "\n",
    "nn_util.plot_mse_bar(mse_norm_dict)\n",
    "nn_util.plot_mse_bar(mse_norm_syn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ov2FjrmseJcsq8lLgAFU6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 11 (easy): Reflection on resultsüí°**\n",
    "1. Experiment with the hyperparameter settings in the `learning_rate`\n",
    " and number of `epoch`\n",
    " lists.\n",
    "2. What are the benefits and cost of training with larger/smaller learning rate? Reflect on the effect of changing the learning rate.\n",
    "3. Reflect on the effect of the loss and training time when changing the number of epochs. \n",
    "4. What is the relationship between learning rate and epochs? \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "cXvbMr7k8MBoLKAHCNbY_",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2mAG8JNVuuBiNX5gIpQKa",
   "metadata": {},
   "source": [
    "### Influence of noise\n",
    "The following steps investigate the effect of noise on the model performance. The files `cleaned_pupils.csv`\n",
    " and `cleaned_screen_coordinates.csv`\n",
    " in the `data`\n",
    " folder, contains cleaned pupil coordinates and their corresponding labels.\n",
    "\n",
    "---\n",
    "**Task 12 (easy): Load data (cleaned gaze data)üë©‚Äçüíª**\n",
    "1. Run the cell below to load the data.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "NG1npt-Cg_oVs981uThKF",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_input = nn_util.load_from_csv('data/cleaned_pupils.csv')\n",
    "cleaned_label = nn_util.load_from_csv('data/cleaned_screen_coordinates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4VsXFmSfCmuE8FYDj7vKi",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 13 (easy): Train a linear model on cleaned gaze dataüë©‚Äçüíª**\n",
    "1. Copy the code from the previous task into the cell below and use the data `cleaned_input`\n",
    " and `cleaned_label`\n",
    ". The cell should:\n",
    "    - Normalize the data. \n",
    "    - Train the models:        - Create two nested for-loops to iterate the lists containing values for `learning rate`\n",
    " and `epochs`\n",
    ". On each iteration the loops should:            - Train the model on the data, using the `train_model`\n",
    " function.\n",
    "            - Test the model using the `test_model`\n",
    " function.\n",
    "            - Save the following information in the designated dictionaries with the suffix `cleaned`\n",
    ":                - Model \n",
    "                - Loss (training)\n",
    "                - Loss (validation)\n",
    "                - Training time \n",
    "                - Predictions\n",
    "                - Errors \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Use the function `plot_results_collected`\n",
    " from the `nn_util.py`\n",
    " file to visualize the result.\n",
    "\n",
    "\n",
    "2. Save the best performing model and the corresponding values specified in the data structure for plotting box into the dictionaries with the suffix `arc`\n",
    ". These will later be used to compare with other architectures.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "Gt-wElvR_MMGfOnS71sqN",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "learning_rate = [0.1]\n",
    "epoch = [2000]\n",
    "criterion = MSELoss()\n",
    "\n",
    "\n",
    "models_dict_arc = {}\n",
    "losses_dict_arc = {}\n",
    "losses_val_dict_arc = {}\n",
    "training_time_dict_arc = {}\n",
    "pred_norm_arc = {}\n",
    "errors_norm_arc = {}\n",
    "mse_arc = {}\n",
    "\n",
    "models_dict_cleaned = {}\n",
    "losses_dict_cleaned = {}\n",
    "losses_val_dict_cleaned = {}\n",
    "training_time_dict_cleaned = {}\n",
    "pred_norm_cl = {}\n",
    "errors_norm_cl = {}\n",
    "\n",
    "\n",
    "\n",
    "for i in learning_rate:\n",
    "    for j in epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IUm_pz3OFDBxsVEw__WXw",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 14 (easy): Reflections on model performanceüí°**\n",
    "1. Given that 39/796 coordinates in the `cleaned`\n",
    " dataset were removed as outliers, reflect on the how the type of outliers influence model performance by comparing the model trained on the uncleaned normalized dataset in task [Task 10](#prediction100) to the model in task [Task 13](#cleaned). Include the following points in your discussion:    - Why do only 39 points significantly affect the model‚Äôs performance?\n",
    "    - How does the division of training and test data influence the model‚Äôs performance?\n",
    "    - What methods could be used to perform an in-depth analysis of such data splits?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "2tjwNLcziae7F-EySXRb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "br7ahCEpKcUys2UAlCyON",
   "metadata": {},
   "source": [
    "## Non-linear Model\n",
    "The following steps are about two different architectures for non-linear models. Compare the non-linear models to the affine model as done above. \n",
    "\n",
    "---\n",
    "**Task 15 (easy): Analyse architectureüí°**\n",
    "1. Examine the cell below to get an overview of the two neural architectures and identify the main differences between the models.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "FvpsZsUYJnSBtMhlS2qiO",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNRelu(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NNRelu, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    " \n",
    "class NNRelu_exp(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NNRelu_exp, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(hidden_dim, output_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0M_5VPNTCqBexyWFNM7cI",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "z8qm-0EsgkOyazPazLmcN",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ps5zg8Fsf4teEgbXlSnpj",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 16 (easy): Train non-linear models (cleaned gaze data)üë©‚Äçüíª**\n",
    "**Note:** For the exam it may be convenient to copy the code from above to the cell below as you complete the steps. \n",
    "\n",
    "1. Rerun task [Task 10](#prediction100) using the normalized, cleaned data, on the two new models. The steps were:\n",
    "\n",
    "- Train the models:\n",
    "    - Create two nested for-loops looping the lists containing values for `learning rate`\n",
    " and `epochs`\n",
    ". The loops should:        - Train models of both architectures on the cleaned normalized data, using the `train_model`\n",
    " function.\n",
    "        - Test the models using the `test_model`\n",
    " function.\n",
    "        - Save the following information in the designated dictionaries with the suffix `arc`\n",
    ", for each model:            - Model \n",
    "            - Loss (training)\n",
    "            - Loss (validation)\n",
    "            - Training time \n",
    "            - Predictions\n",
    "            - Errors \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Use the function `plot_results_collected`\n",
    " from the `nn_util.py`\n",
    " file, to visualize the result.\n",
    "\n",
    "- Use the function `plot_mse_bar`\n",
    " from the `nn_util.py`\n",
    " file, to visualize the mean squared error compared.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "F6DEFr_p2xdARgIhQizW7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "learning_rate = [0.001, 0.1, 1.5]\n",
    "epoch = [500, 2000, 10000]\n",
    "hidden_layer = 10\n",
    "\n",
    "criterion = MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in learning_rate:\n",
    "    for j in epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y0ul35ASHEcB9c_0Qntv3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 17 (easy): Reflect on the resultsüí°**\n",
    "1. Do more/less complex models improve the result? Why/why not?\n",
    "2. Are there indications of overfitting in any of the models?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "xwcaRaZPVIKcXcA7MJ8vw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2gmm3jKfD63G-bO4xtgWn",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Task 18 (easy): Analyse resultsüí°**\n",
    "1. Experiment with other architectures by suggesting models with different number layers and neurons in each layer.\n",
    "**Note:** This part of the exercise can easily become a timesink, mind your time as you proceed.\n",
    "\n",
    "\n",
    "2. Reflect on the results    - Do more/less complex models improve the result? Why/why not?\n",
    "    - Do any of the models show signs of overfitting?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "2Fhhp5HSB4mnU8YY6jfg3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3vnliksWf41jemuBKLmN",
   "metadata": {},
   "source": [
    "## Own dataset\n",
    "You are now encouraged to experiment with your own dataset and other models. \n",
    "**Note:** For the exam it may be convenient to copy your code from task [Task 10](#prediction100) to the cell below.\n",
    "This part of the exercise can easily become a timesink, mind your time as you proceed.\n",
    "\n",
    "\n",
    "---\n",
    "**Task 19 (easy): Train models (own dataset)üë©‚Äçüíªüí°**\n",
    "1. Experiment with your own cleaned dataset from the exercise Filtering gaze data\n",
    ". \n",
    "2. Reflect on your results, compare to the results of `test_subject_3`\n",
    ". \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "SEGlhDL00e5Pltbm9RIuq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your reflections here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j4kiGcvLicSX601yI8s8G",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
