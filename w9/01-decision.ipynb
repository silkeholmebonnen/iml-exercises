{
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": "py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "code",
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "@import \"https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css\";\n",
        "</style>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Ic6UA56CFu_96sbNaoRgI",
      "metadata": {},
      "source": [
        "# Classification and decision boundaries\n",
        "This exercise is about linear classification (actually affine) and the visualization of decision boundaries. Noteably, the parameters of the decision boundary will initially be adjusted manually (or randomly) then learned using least squares.\n",
        "\n",
        "**Note**\n",
        "The term linear classification is actually referring to an affine model as it includes a bias term.\n",
        "\n",
        "\n",
        "<article class=\"message\">\n",
        "    <div class=\"message-body\">\n",
        "        <strong>List of individual tasks</strong>\n",
        "        <ul style=\"list-style: none;\">\n",
        "            <li>\n",
        "            <a href=\"#linear\">Task 1: Linear decision boundary</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#predict22\">Task 2: Prediction function - reflections</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#predict\">Task 3: Prediction function</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#linear_optimization\">Task 4: Learning the decision boundary</a>\n",
        "            </li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</article>\n",
        "\n",
        "## Linear Decision boundary\n",
        "The prediction function $f_{w}(\\mathbf{x})$ is used to predict which class a data point belongs to, by applying the $\\text{sign}$ function to the result of the linear combination of input features and weights. The classification is based on whether the output of $f_{w}(\\mathbf{x})$ is positive or negative. \n",
        "### Generating data points\n",
        "In the following section you will be experimenting with a linear classifier: \n",
        "\n",
        "$$ f_{w}(\\mathbf{x}) =  \\mathbf{y} = \\text{sign}({w_0} + \\mathbf{w}^\\top\\mathbf{x})$$\n",
        "where $\\mathbf{w}$ are the model parameters (including a bias term) and $\\mathbf{x}$ are the coordinates of the input. \n",
        "The $\\text{sign}$ function is given by:\n",
        "\n",
        "$$\n",
        "\\text{sign}(z) =\n",
        "\\begin{cases} \n",
        "-1 & \\text{if } z ‚â§ 0, \\\\\n",
        "1  & \\text{if } z > 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "Alternatively, using homogeneous representation the classifier is expressed as (with appropriate updates to $\\mathbf{w}$ and $\\mathbf{x}$):\n",
        "\n",
        "$$\\mathbf{y} = \\text{sign}(\\mathbf{w}^\\top\\mathbf{x})$$\n",
        "### The Decision Boundary\n",
        "In binary classification the decision boundary separates the positive and negative classes and is defined by:\n",
        "\n",
        "$$\n",
        "f_w(\\mathbf{x}) = 0\n",
        "$$\n",
        "Points on one side (positive) of the boundary will be classified as the positive class (1), while points on the other side (negative) will be classified as the negative class (-1).\n",
        "For the affine model, the decision boundary is the line when: \n",
        "\n",
        "$$\n",
        "\\mathbf{w}^\\top \\mathbf{x} + w_0 = 0\n",
        "$$\n",
        "For a two-dimensional affine model (with features $x_1$ and $x_2$), the decision boundary is given by:\n",
        "\n",
        "$$\n",
        "w_1 x_1 + w_2 x_2 + w_0 = 0\n",
        "$$\n",
        "For display purposes, the decision boundary can be expressed in terms of $x_1$ and $x_2$ by isolating $x_2$ on one side:\n",
        "\n",
        "$$\n",
        "x_2 = -\\frac{w_0}{w_2} - \\frac{w_1}{w_2} x_1\n",
        "$$\n",
        "The cell below imports libraries and generates random data to be used for classification. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "YZCrTy1Ki2u4E864Kvowu",
      "metadata": {},
      "source": [
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt  \n",
        "import seaborn as sns  \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "np.random.seed(42)  ## generate the same sequence of random points\n",
        "# Generate 2 clusters of data, by drawing from a normal distribution.\n",
        "S = np.eye(2)  ## covariance matrix, set to indenty matrix i.e. x,y independent. \n",
        "p_pos = np.random.multivariate_normal([1, 1], S, 40)\n",
        "p_neg = np.random.multivariate_normal([-1, -1], S, 40)\n",
        "## 40 points (x,y) coordinates\n",
        "p_pos.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "RjNxUiijMS2pYcSWzdiMk",
      "metadata": {},
      "source": [
        "The data of the positive and negative classes are stored in the variables `p_pos`\n",
        " and `p_neg`\n",
        ", respectively. The next cell visualizes the two classes. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "id": "YTM4u2sIZfzcKTn-ihB-n",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(p_pos[:, 0], p_pos[:, 1], \"o\", label='positive class')\n",
        "ax.plot(p_neg[:, 0], p_neg[:, 1], \"P\", label='negative class')\n",
        "plt.title(\"Data\", fontsize=24)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "kVpBdf8BuTymnJLV_N5Qi",
      "metadata": {},
      "source": [
        "### Setting the model parameters manually\n",
        "In the following task you will manually change the model parameters of a linear decision boundary and visualize the results.\n",
        "\n",
        "---\n",
        "**Task 1 (easy): Linear decision boundaryüë©‚Äçüíª**\n",
        "1. Implement the function `linear_boundary`\n",
        ", which, given an $x_1$-coordinate and the model parameters $w$ = [$w_0$, $w_1$, $w_2$], returns the corresponding $x_2$-value according to:\n",
        "\n",
        "\n",
        "$$\n",
        "x_2 = -\\frac{w_0}{w_2} - \\frac{w_1}{w_2} x_1\n",
        "$$\n",
        "2. The array `x_values`\n",
        " below provides the $x_1$-values (x-coordinates) over which the boundary will be plotted. The model parameters $w_0$, $w_1$, and $w_2$ define the position and slope of the boundary. Use the `linear_boundary`\n",
        " function to generate points for the decision boundary by implementing the following steps:\n",
        "\n",
        "- Create an array `w`\n",
        " with the manually selected model parameters. \n",
        "- Pass the `x_values`\n",
        " and `w`\n",
        " to the `linear_boundary`\n",
        " function to calculate the corresponding $x_2$-values.\n",
        "\n",
        "3. Run the cell below to visualize the decision boundary. Which choice of model parameters $w_0$, $w_1$, and $w_2$ seems to visually best separate the two classes? Try 10 different sets of model parameters and identify which values provide the largest fraction of correct predictions.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "m5fj_KiwkyLKCkx5yS9Yq",
      "metadata": {},
      "source": [
        "def linear_boundary(x, w):\n",
        "    \"\"\"\n",
        "    :param x: x values of the line.\n",
        "    :param w: List of model parameters [bias, slope] of the line.\n",
        "    \n",
        "    :return: the x2 values which correspond to the y-values of the boundary / line .\n",
        "    \"\"\"\n",
        "    # Write solutions here\n",
        "    ...\n",
        "\n",
        "# Defining x-values and weights\n",
        "x_values = np.linspace(-3, 3, 100)\n",
        "w = None # write your solution here\n",
        "\n",
        "# Plotting the data points and decision boundary\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(p_pos[:, 0], p_pos[:, 1], \"o\", label='Positive class')\n",
        "ax.plot(p_neg[:, 0], p_neg[:, 1], \"P\", label='Negative class')\n",
        "ax.plot(x_values, linear_boundary(x_values, w))\n",
        "plt.title(\"2D Linear Decision Boundary\", fontsize=20)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "hjeL3FU8QcEaG2rG3TiWX",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 2 (easy): Prediction function - reflectionsüí°**\n",
        "1. Is a linear decision boundary a good model to separate the two groups of data? \n",
        "2. A correct prediction occurs when  $y_i \\cdot f_w(\\mathbf{x}_i)$  is positive, where  $y_i$  is the true label for the $i$-th data point and  $\\mathbf{x}_i$  is the input vector for the $i$-th data point. Why is this the case?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "A3sSmX0qYXe1eM7dKZxQE",
      "metadata": {},
      "source": [
        "# Write your reflections here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "MA4vHiSSeP5vFC_CLV4UI",
      "metadata": {},
      "source": [
        "### Making and evaluating predictions\n",
        "The performance of the model can be evaluated by calculating the _accuracy_, which is defined as (percentage): \n",
        "$$ \\text{accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
        ".\n",
        "\n",
        "---\n",
        "**Task 3 (easy): Prediction functionüë©‚Äçüíª**\n",
        "1. Implement the function `predict`\n",
        " that given the data and the model parameters `w`\n",
        " as input, predicts whether a data point belongs to the `neg`\n",
        " or `pos`\n",
        " class. Return 1 for points above the boundary (positive class) and -1 for points below the boundary (negative class). Manually select the model parameters to predict class labels for both the `p_neg`\n",
        " and `p_pos`\n",
        " variables by calculating the sign of the prediction function  $f_w(\\mathbf{x})=z = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2$  for each data point. \n",
        "\n",
        "2. Implement and execute the `accuracy`\n",
        " function that returns the accuracy of the classifier over the entire training set by comparing the predicted classes to the actual classes.\n",
        "\n",
        "\n",
        "\n",
        "**Tip**\n",
        "The denominator in the accuracy formula can be found by counting the number of times the prediction $f_w(\\mathbf{x_i}) = y_i$, or the amount of times $ y_i \\cdot f_w(\\mathbf{x_i}) > 0 $,  where  $y_i$ are the labels.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "KK7Tmuwkk4FdiI-mjAY_u",
      "metadata": {},
      "source": [
        "def predict(w, p):\n",
        "    \"\"\"\n",
        "    Predict class based on decision boundary (logistic regression).\n",
        "    :param w: Model parameters [w0, w1, w2].\n",
        "    :param p: Data points to classify (Nx2 array).\n",
        "    \n",
        "    :return: Predicted class labels (-1 for neg, 1 for pos).\n",
        "    \"\"\"\n",
        "    # Write solutions here\n",
        "\n",
        "\n",
        "def accuracy(predictions,targets):\n",
        "    \"\"\"\n",
        "    :param predictions (1xn array): of predicted classes for the n data points.\n",
        "    :param targets (1xn array):  actual classes for the n data points.\n",
        "     \n",
        "    :return (float): fraction of correctly predicted points (num_correct/num_points).\n",
        "    \"\"\"\n",
        "    # Write solutions here\n",
        "\n",
        "\n",
        "pred_pos = None # write your solution here\n",
        "pred_neg = None # write your solution here\n",
        "\n",
        "acc_pos = None # write your solution here\n",
        "acc_neg = None # write your solution here\n",
        "\n",
        "print(f'Accuracy of decision line: {(acc_neg+acc_pos)/2:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "XGFIGJGlCm6bA5gCgr0wV",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 4 (easy): Learning the decision boundaryüë©‚Äçüíª**\n",
        "1. Implement the `learn_affine_classifier`\n",
        " function which given arrays of positive and negative examples uses least squares to estimate the model parameters `w`\n",
        " to separate the two classes. In particular the function should:\n",
        "    - Construct the $N \\times 3$ design matrix containing the data points.\n",
        "    - Construct the vector `labels`\n",
        " consisting of the labels  (1 for positive class, -1 for negative class).\n",
        "    - Solve the linear set of equations.\n",
        "\n",
        "\n",
        "2. Run the cell below to plot the decision boundary. \n",
        "\n",
        "3. Evaluate the accuracy of the learned decision boundary: \n",
        "    - Use the `predict`\n",
        " function to predict class labels for the data points. \n",
        "    - Use the `accuracy`\n",
        " function to evaluate the found model.\n",
        "    - Compare the decision boundary and the accuracy measures to the one obtained in [Task 3](#predict). How do the values found manually compare to the one found automatically through least squares?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "id": "HWWLpMNYb_Qj2r65Bt1NC",
      "metadata": {},
      "source": [
        "# 1\n",
        "def learn_affine_classifier(p_pos, p_neg):\n",
        "    \"\"\"\n",
        "    Fit the decision boundary using least squares regression.\n",
        "    \n",
        "    :param p_pos: Data points of the positive class.\n",
        "    :param p_neg: Data points of the negative class.\n",
        "    \n",
        "    :return: The learned parameters w (intercept and slope).\n",
        "    \"\"\"\n",
        "    # Combine the positive and negative points into a single dataset\n",
        "    data = None # Write your solution here\n",
        "    # Create the labels: 1 for positive class, 0 for negative class\n",
        "    labels = None # Write your solution here\n",
        "    # Add a bias column (column of 1s) to the data\n",
        "    X = None # Write your solution here\n",
        "    # Learn the model parameters\n",
        "    w_least_squares = None # Write your solution here\n",
        "    # Return the model parameters\n",
        "    return w_least_squares\n",
        "\n",
        "# Find the decision boundary with least squares\n",
        "w_learned = learn_affine_classifier(p_pos, p_neg)\n",
        "\n",
        "# 2\n",
        "# Define x-values for plotting the decision boundaries\n",
        "x_values = np.linspace(-3, 3, 100)\n",
        "\n",
        "# Plotting the learned decision boundary\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(p_pos[:, 0], p_pos[:, 1], marker='o', label='Class 1')  # positive class\n",
        "ax.scatter(p_neg[:, 0], p_neg[:, 1], marker='x', label='Class 2')  # negative class\n",
        "ax.plot(x_values, linear_boundary(x_values, w_learned), label='Learned boundary: y = %.2f + %.2f x' % (w_learned[0], w_learned[1]), color='green')\n",
        "plt.title(\"Learned Decision Boundary\", fontsize=20)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 3\n",
        "pred_pos = None # write your solution here\n",
        "pred_neg = None # write your solution here\n",
        "\n",
        "acc_pos = None # write your solution here\n",
        "acc_neg = None # write your solution here\n",
        "\n",
        "print(f'Accuracy of learned boundary: {(acc_neg + acc_pos) / 2:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Soq0L2CQfW0d0JRCznLOa",
      "metadata": {},
      "source": [
        ""
      ]
    }
  ]
}